import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures
from sklearn.linear_model import Ridge, Lasso, LinearRegression
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.api as sm

# Step 1: Load Data
data = pd.read_csv('/mnt/data/london_houses.csv')

# Initial Exploration
print(data.info())
print(data.describe())

# Visualize Distribution of Price
plt.figure(figsize=(10, 6))
sns.histplot(data['Price (£)'], kde=True, color='blue', bins=30)
plt.title('Distribution of House Prices')
plt.xlabel('Price (£)')
plt.ylabel('Frequency')
plt.show()

# Step 2: Check for Missing Values
sns.heatmap(data.isnull(), cbar=False, cmap='viridis')
plt.title('Missing Values Heatmap')
plt.show()

# Step 3: Outlier Detection
plt.figure(figsize=(8, 5))
sns.boxplot(data['Price (£)'])
plt.title('Boxplot of House Prices')
plt.xlabel('Price (£)')
plt.show()

# Remove Outliers using IQR
Q1 = data['Price (£)'].quantile(0.25)
Q3 = data['Price (£)'].quantile(0.75)
IQR = Q3 - Q1
data = data[~((data['Price (£)'] < (Q1 - 1.5 * IQR)) | (data['Price (£)'] > (Q3 + 1.5 * IQR)))]

# Step 4: Feature Engineering
data['AgeOfBuilding'] = 2025 - data['Building Age']
data['Log_Price'] = np.log(data['Price (£)'])
data['TransportAccessibility'] = np.where(data['Floors'] < 3, 1, 0)

plt.figure(figsize=(10, 6))
sns.scatterplot(x=data['Square Meters'], y=data['Price (£)'], hue=data['Neighborhood'])
plt.title('Price vs. Square Meters')
plt.xlabel('Square Meters')
plt.ylabel('Price (£)')
plt.show()

# Step 5: Encoding Categorical Variables
encoder = OneHotEncoder(sparse=False, drop='first')
categorical_columns = ['Neighborhood', 'Property Type']
encoded_data = pd.DataFrame(
    encoder.fit_transform(data[categorical_columns]), 
    columns=encoder.get_feature_names_out(categorical_columns)
)
data = pd.concat([data, encoded_data], axis=1)
data.drop(columns=categorical_columns, inplace=True)

# Step 6: Feature Selection and Multicollinearity
numerical_columns = ['Square Meters', 'Bathrooms', 'Bedrooms', 'Floors', 'AgeOfBuilding']
X = data[numerical_columns + encoded_data.columns.tolist()]
y = data['Log_Price']

# Calculate VIF
X_scaled = pd.DataFrame(StandardScaler().fit_transform(X), columns=X.columns)
vif_data = pd.DataFrame()
vif_data['Feature'] = X_scaled.columns
vif_data['VIF'] = [variance_inflation_factor(X_scaled.values, i) for i in range(X_scaled.shape[1])]
print(vif_data)

# Step 7: Scaling Features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 8: Model Building
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

ridge = Ridge()
lasso = Lasso(max_iter=10000)
gb = GradientBoostingRegressor(random_state=42)

ridge_params = {'alpha': [0.01, 0.1, 1, 10, 100]}
lasso_params = {'alpha': [0.001, 0.01, 0.1, 1, 10]}
ridge_grid = GridSearchCV(ridge, ridge_params, cv=5, scoring='neg_mean_squared_error')
lasso_grid = GridSearchCV(lasso, lasso_params, cv=5, scoring='neg_mean_squared_error')

ridge_grid.fit(X_train, y_train)
lasso_grid.fit(X_train, y_train)
gb.fit(X_train, y_train)

# Best Hyperparameters
print("Best Ridge Alpha:", ridge_grid.best_params_)
print("Best Lasso Alpha:", lasso_grid.best_params_)

# Predictions
y_pred_gb = gb.predict(X_test)

# Evaluate Models
ridge_rmse = np.sqrt(-ridge_grid.best_score_)
lasso_rmse = np.sqrt(-lasso_grid.best_score_)
gb_rmse = mean_squared_error(y_test, y_pred_gb, squared=False)
gb_r2 = r2_score(y_test, y_pred_gb)

print("Ridge RMSE:", ridge_rmse)
print("Lasso RMSE:", lasso_rmse)
print("Gradient Boosting RMSE:", gb_rmse)
print("Gradient Boosting R2:", gb_r2)

# Step 9: Visualizations
# Feature Importance
feature_importance = gb.feature_importances_
sorted_idx = np.argsort(feature_importance)[::-1]
plt.figure(figsize=(12, 8))
plt.barh(np.array(X.columns)[sorted_idx[:10]][::-1], feature_importance[sorted_idx[:10]][::-1])
plt.xlabel('Feature Importance')
plt.title('Top 10 Features - Gradient Boosting')
plt.show()

# Residual Plot
residuals = y_test - y_pred_gb
plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_pred_gb, y=residuals, alpha=0.7)
plt.axhline(y=0, color='red', linestyle='--')
plt.title('Residuals vs Predicted Prices')
plt.xlabel('Predicted Prices (Log)')
plt.ylabel('Residuals')
plt.show()

# Actual vs Predicted Prices
plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_test, y=y_pred_gb, alpha=0.7)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')
plt.title('Actual vs Predicted Prices')
plt.xlabel('Actual Prices (Log)')
plt.ylabel('Predicted Prices (Log)')
plt.show()
